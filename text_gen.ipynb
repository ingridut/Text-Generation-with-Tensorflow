{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset \n",
    "\n",
    "Upload dataset and add path. Check that the length of the text is as expected. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Textfile Iliad_v3.tx is 153260 words long\n",
      "Text Sample: \n",
      "  achilles' wrath, to greece the direful spring\n",
      "  of woes unnumber'd, heavenly goddess, sing!\n",
      "  that\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "path = os.getcwd() + '/Data'\n",
    "file_name = '/Iliad_v3.txt'\n",
    "\n",
    "text = open(path + file_name, 'rb').read().decode(encoding='utf-8')\n",
    "words = [w for w in text.split(' ') if w.strip() != '' or w == '\\n']\n",
    "print(\"Textfile {0} is {1} words long\".format(file_name[1:-1], len(words)))\n",
    "\n",
    "print('Text Sample: ')\n",
    "print(text[:100])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the text\n",
    "\n",
    "Prepare the text by mapping unique characters to indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 34 unique characters\n",
      "Character vector with mapping:\n",
      "\n",
      " '\\n':   0,\n",
      " ' ' :   1,\n",
      " '!' :   2,\n",
      " \"'\" :   3,\n",
      " ',' :   4,\n",
      " '-' :   5,\n",
      " '.' :   6,\n",
      " '?' :   7,\n",
      " 'a' :   8,\n",
      " 'b' :   9,\n",
      " 'c' :  10,\n",
      " 'd' :  11,\n",
      " 'e' :  12,\n",
      " 'f' :  13,\n",
      " 'g' :  14,\n",
      " 'h' :  15,\n",
      " 'i' :  16,\n",
      " 'j' :  17,\n",
      " 'k' :  18,\n",
      " 'l' :  19,\n",
      " 'm' :  20,\n",
      " 'n' :  21,\n",
      " 'o' :  22,\n",
      " 'p' :  23,\n",
      " 'q' :  24,\n",
      " 'r' :  25,\n",
      " 's' :  26,\n",
      " 't' :  27,\n",
      " 'u' :  28,\n",
      " 'v' :  29,\n",
      " 'w' :  30,\n",
      " 'x' :  31,\n",
      " 'y' :  32,\n",
      " 'z' :  33,\n"
     ]
    }
   ],
   "source": [
    "# Find all unique characters\n",
    "vocab = sorted(set(text))\n",
    "print ('There are {} unique characters'.format(len(vocab)))\n",
    "\n",
    "# Create array with the characters and the index they are mapped to\n",
    "char2int = {c:i for i, c in enumerate(vocab)}\n",
    "int2char = np.array(vocab)\n",
    "print('Character vector with mapping:\\n')\n",
    "for char,_ in zip(char2int, range(len(vocab))):\n",
    "   print(' {:4s}: {:3d},'.format(repr(char), char2int[char]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  achilles' wrath, to greece the direful spring\n",
      "  of woes unnumber'd, heavenly goddess, sing!\n",
      "  that\n",
      " mapped to integers:\n",
      " [ 1  1  8 10 15 16 19 19 12 26  3  1 30 25  8 27 15  4  1 27 22  1 14 25\n",
      " 12 12 10 12  1 27 15 12  1 11 16 25 12 13 28 19  1 26 23 25 16 21 14  0\n",
      "  1  1 22 13  1 30 22 12 26  1 28 21 21 28 20  9 12 25  3 11  4  1 15 12\n",
      "  8 29 12 21 19 32  1 14 22 11 11 12 26 26  4  1 26 16 21 14  2  0  1  1\n",
      " 27 15  8 27]\n"
     ]
    }
   ],
   "source": [
    "# Map text to integers\n",
    "text_as_int = np.array([char2int[ch] for ch in text], dtype=np.int32)\n",
    "print ('{}\\n mapped to integers:\\n {}'.format((text[:100]), text_as_int[:100]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separate data for training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_text = text_as_int[:704000] #text separated for training, divisible by the batch size (64)\n",
    "val_text = text_as_int[704000:] #text separated for validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Populate the library of tunables - I like keeping them centralized in case I need to change things around:\n",
    "batch_size = 64\n",
    "buffer_size = 10000\n",
    "embedding_dim = 256\n",
    "epochs = 50\n",
    "seq_length = 200\n",
    "examples_per_epoch = len(text)//seq_length\n",
    "#lr = 0.001 #will use default for Adam optimizer\n",
    "rnn_units = 1024\n",
    "vocab_size = len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_char_dataset = tf.data.Dataset.from_tensor_slices(tr_text)\n",
    "val_char_dataset = tf.data.Dataset.from_tensor_slices(val_text)\n",
    "\n",
    "tr_sequences = tr_char_dataset.batch(seq_length+1, drop_remainder=True)\n",
    "val_sequences = val_char_dataset.batch(seq_length+1, drop_remainder=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<BatchDataset shapes: ((64, 200), (64, 200)), types: (tf.int32, tf.int32)> <BatchDataset shapes: ((64, 200), (64, 200)), types: (tf.int32, tf.int32)>\n"
     ]
    }
   ],
   "source": [
    "def split_input_target(chunk):\n",
    "    input_text = chunk[:-1]\n",
    "    target_text = chunk[1:]\n",
    "    return input_text, target_text\n",
    "\n",
    "tr_dataset = tr_sequences.map(split_input_target).shuffle(buffer_size).batch(batch_size, drop_remainder=True)\n",
    "val_dataset = val_sequences.map(split_input_target).shuffle(buffer_size).batch(batch_size, drop_remainder=True)\n",
    "print(tr_dataset, val_dataset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TF2-RNN",
   "language": "python",
   "name": "tf2-rnn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
